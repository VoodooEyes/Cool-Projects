{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "nbTranslate": {
      "displayLangs": [
        "*"
      ],
      "hotkey": "alt-t",
      "langInMainMenu": true,
      "sourceLang": "en",
      "targetLang": "fr",
      "useGoogleTranslate": true
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "Imprint_Task.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4d1Z-XPwBoY",
        "colab_type": "text"
      },
      "source": [
        "# Imprint Data Science Exercise - Reddit Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RF8K9o3OwBpB",
        "colab_type": "text"
      },
      "source": [
        "### Pip install cell"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T21:33:41.901195Z",
          "start_time": "2020-01-10T21:24:44.648619Z"
        },
        "id": "Xgah8XZcwBpK",
        "colab_type": "code",
        "outputId": "de0e2b8c-8816-4699-aaf5-ee9a6da154a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#wrapper for reddit api\n",
        "!pip install praw\n",
        "#keras based library that help uses BERT\n",
        "!pip install ktrain\n",
        "#flask for serving the model\n",
        "!pip install flask\n",
        "!pip install flask-ngrok"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting praw\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/25/c0/b9714b4fb164368843b41482a3cac11938021871adf99bf5aaa3980b0182/praw-6.5.1-py3-none-any.whl (134kB)\n",
            "\r\u001b[K     |██▍                             | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |████▉                           | 20kB 1.5MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 30kB 2.1MB/s eta 0:00:01\r\u001b[K     |█████████▊                      | 40kB 1.6MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 51kB 1.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 71kB 2.3MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 81kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 92kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 102kB 2.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 112kB 2.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 122kB 2.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 133kB 2.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 143kB 2.7MB/s \n",
            "\u001b[?25hCollecting update-checker>=0.16\n",
            "  Downloading https://files.pythonhosted.org/packages/17/c9/ab11855af164d03be0ff4fddd4c46a5bd44799a9ecc1770e01a669c21168/update_checker-0.16-py2.py3-none-any.whl\n",
            "Collecting prawcore<2.0,>=1.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/76/b5/ce6282dea45cba6f08a30e25d18e0f3d33277e2c9fcbda75644b8dc0089b/prawcore-1.0.1-py2.py3-none-any.whl\n",
            "Collecting websocket-client>=0.54.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4c/5f/f61b420143ed1c8dc69f9eaec5ff1ac36109d52c80de49d66e0c36c3dfdf/websocket_client-0.57.0-py2.py3-none-any.whl (200kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 9.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from update-checker>=0.16->praw) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from websocket-client>=0.54.0->praw) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.3.0->update-checker>=0.16->praw) (3.0.4)\n",
            "Installing collected packages: update-checker, prawcore, websocket-client, praw\n",
            "Successfully installed praw-6.5.1 prawcore-1.0.1 update-checker-0.16 websocket-client-0.57.0\n",
            "Collecting ktrain\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/2e/11a72b91bfd73bbb5ac215870cc8899a1f6df622559e532d7af437f5bc35/ktrain-0.7.3.tar.gz (89kB)\n",
            "\u001b[K     |████████████████████████████████| 92kB 2.7MB/s \n",
            "\u001b[?25hCollecting scikit-learn==0.21.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "\u001b[K     |████████████████████████████████| 6.7MB 8.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: matplotlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from ktrain) (3.1.2)\n",
            "Requirement already satisfied: pandas>=0.24.2 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.25.3)\n",
            "Requirement already satisfied: fastprogress>=0.1.21 in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.2.2)\n",
            "Collecting keras_bert\n",
            "  Downloading https://files.pythonhosted.org/packages/df/fe/bf46de1ef9d1395cd735d8df5402f5d837ef82cfd348a252ad8f32feeaef/keras-bert-0.80.0.tar.gz\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from ktrain) (2.21.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.14.1)\n",
            "Collecting langdetect\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/59/4bc44158a767a6d66de18c4136c8aa90491d56cc951c10b74dd1e13213c9/langdetect-1.0.7.zip (998kB)\n",
            "\u001b[K     |████████████████████████████████| 1.0MB 36.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: jieba in /usr/local/lib/python3.6/dist-packages (from ktrain) (0.40)\n",
            "Collecting cchardet\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/fa/4e/847feebfc3e71c773b23ee06c74687b8c50a5a6d6aaff452a0a4f4eb9a32/cchardet-2.1.5-cp36-cp36m-manylinux1_x86_64.whl (241kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 40.5MB/s \n",
            "\u001b[?25hCollecting networkx==2.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
            "\u001b[K     |████████████████████████████████| 1.8MB 33.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: bokeh in /usr/local/lib/python3.6/dist-packages (from ktrain) (1.4.0)\n",
            "Collecting seqeval\n",
            "  Downloading https://files.pythonhosted.org/packages/34/91/068aca8d60ce56dd9ba4506850e876aba5e66a6f2f29aa223224b50df0de/seqeval-0.0.12.tar.gz\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from ktrain) (20.0)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn==0.21.3->ktrain) (1.17.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.4.6)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib>=3.0.0->ktrain) (2.6.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.24.2->ktrain) (2018.9)\n",
            "Requirement already satisfied: Keras in /usr/local/lib/python3.6/dist-packages (from keras_bert->ktrain) (2.2.5)\n",
            "Collecting keras-transformer>=0.30.0\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/57/496b1eab888171b0801a0a44d3245a7874b8d1cc04c1fbfdbb5e3327fc7a/keras-transformer-0.31.0.tar.gz\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (3.0.4)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->ktrain) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from langdetect->ktrain) (1.12.0)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx==2.3->ktrain) (4.4.1)\n",
            "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (6.2.2)\n",
            "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (2.10.3)\n",
            "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (3.13)\n",
            "Requirement already satisfied: tornado>=4.3 in /usr/local/lib/python3.6/dist-packages (from bokeh->ktrain) (4.5.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from kiwisolver>=1.0.1->matplotlib>=3.0.0->ktrain) (42.0.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (2.8.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from Keras->keras_bert->ktrain) (1.0.8)\n",
            "Collecting keras-pos-embd>=0.10.0\n",
            "  Downloading https://files.pythonhosted.org/packages/09/70/b63ed8fc660da2bb6ae29b9895401c628da5740c048c190b5d7107cadd02/keras-pos-embd-0.11.0.tar.gz\n",
            "Collecting keras-multi-head>=0.22.0\n",
            "  Downloading https://files.pythonhosted.org/packages/40/3e/d0a64bb2ac5217928effe4507c26bbd19b86145d16a1948bc2d4f4c6338a/keras-multi-head-0.22.0.tar.gz\n",
            "Collecting keras-layer-normalization>=0.12.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a4/0e/d1078df0494bac9ce1a67954e5380b6e7569668f0f3b50a9531c62c1fc4a/keras-layer-normalization-0.14.0.tar.gz\n",
            "Collecting keras-position-wise-feed-forward>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/e3/59/f0faa1037c033059e7e9e7758e6c23b4d1c0772cd48de14c4b6fd4033ad5/keras-position-wise-feed-forward-0.6.0.tar.gz\n",
            "Collecting keras-embed-sim>=0.7.0\n",
            "  Downloading https://files.pythonhosted.org/packages/bc/20/735fd53f6896e2af63af47e212601c1b8a7a80d00b6126c388c9d1233892/keras-embed-sim-0.7.0.tar.gz\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.7->bokeh->ktrain) (1.1.1)\n",
            "Collecting keras-self-attention==0.41.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1b/1c/01599219bef7266fa43b3316e4f55bcb487734d3bafdc60ffd564f3cfe29/keras-self-attention-0.41.0.tar.gz\n",
            "Building wheels for collected packages: ktrain, keras-bert, langdetect, networkx, seqeval, keras-transformer, keras-pos-embd, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-self-attention\n",
            "  Building wheel for ktrain (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ktrain: filename=ktrain-0.7.3-cp36-none-any.whl size=113951 sha256=bcd783b1c80d634ce4db5c43b57ef7455b5acee2ddacdd68d07eccfbef18a0b3\n",
            "  Stored in directory: /root/.cache/pip/wheels/86/20/70/000142264c6a768a4e8b67d8ef2ebc32c018684acb2eed7586\n",
            "  Building wheel for keras-bert (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-bert: filename=keras_bert-0.80.0-cp36-none-any.whl size=37923 sha256=3a442d3f71fe9c71ce10dd1dbe3023148db31fb201d6f8de5bd0e5ed730b6f2a\n",
            "  Stored in directory: /root/.cache/pip/wheels/63/dc/87/3260cb91f3aa32c0f85c5375429a30c8fd988bbb48f5ee21b0\n",
            "  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langdetect: filename=langdetect-1.0.7-cp36-none-any.whl size=993460 sha256=6847dc2d9160e4b59807a547235821aabc5409b0b7fa08c9e857ede496f483be\n",
            "  Stored in directory: /root/.cache/pip/wheels/ec/0c/a9/1647275e7ef5014e7b83ff30105180e332867d65e7617ddafe\n",
            "  Building wheel for networkx (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for networkx: filename=networkx-2.3-py2.py3-none-any.whl size=1556408 sha256=7ac09642fc1f9facca1f4046cf3b672116b12affcf40758b8861548085d83b96\n",
            "  Stored in directory: /root/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
            "  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for seqeval: filename=seqeval-0.0.12-cp36-none-any.whl size=7424 sha256=84820cc9863219885cccb1183c93724b8d2951821e6e0c2dbeb5f00eea6b57e5\n",
            "  Stored in directory: /root/.cache/pip/wheels/4f/32/0a/df3b340a82583566975377d65e724895b3fad101a3fb729f68\n",
            "  Building wheel for keras-transformer (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-transformer: filename=keras_transformer-0.31.0-cp36-none-any.whl size=13385 sha256=8995632c0ffd04b183881e3e7fef479487fab002b267c15e7b804a9562d7acc3\n",
            "  Stored in directory: /root/.cache/pip/wheels/a3/c5/9a/5a5130240be614a7a6fa786765d7692ae97f82601e2161bb56\n",
            "  Building wheel for keras-pos-embd (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-pos-embd: filename=keras_pos_embd-0.11.0-cp36-none-any.whl size=7553 sha256=1aef476959354a157356d48b39f8b4dbf5dbc7505524104a3011da35312cdc04\n",
            "  Stored in directory: /root/.cache/pip/wheels/5b/a1/a0/ce6b1d49ba1a9a76f592e70cf297b05c96bc9f418146761032\n",
            "  Building wheel for keras-multi-head (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-multi-head: filename=keras_multi_head-0.22.0-cp36-none-any.whl size=15371 sha256=b17d7f62ad66be110375a17c33f162fb36787b8106ad7db8ba4e9334cb2064e7\n",
            "  Stored in directory: /root/.cache/pip/wheels/bb/df/3f/81b36f41b66e6a9cd69224c70a737de2bb6b2f7feb3272c25e\n",
            "  Building wheel for keras-layer-normalization (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-layer-normalization: filename=keras_layer_normalization-0.14.0-cp36-none-any.whl size=5268 sha256=f3014c640f8e4f72cf92f0b9bf554deb71339ce5cf344253c84c10dd96189abb\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/80/22/a638a7d406fd155e507aa33d703e3fa2612b9eb7bb4f4fe667\n",
            "  Building wheel for keras-position-wise-feed-forward (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-position-wise-feed-forward: filename=keras_position_wise_feed_forward-0.6.0-cp36-none-any.whl size=5624 sha256=cb3fce56d120279120d746f4bda64e984f2a6da4bfa392a4ef70afc2613d3959\n",
            "  Stored in directory: /root/.cache/pip/wheels/39/e2/e2/3514fef126a00574b13bc0b9e23891800158df3a3c19c96e3b\n",
            "  Building wheel for keras-embed-sim (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-embed-sim: filename=keras_embed_sim-0.7.0-cp36-none-any.whl size=4676 sha256=b41205425bb403cc210e3541d0b24521bc63d24b4d1634746005e5ae6be6e1d8\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/bc/b1/b0c45cee4ca2e6c86586b0218ffafe7f0703c6d07fdf049866\n",
            "  Building wheel for keras-self-attention (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for keras-self-attention: filename=keras_self_attention-0.41.0-cp36-none-any.whl size=17290 sha256=05e8e33119325de6d007fb40dfea91f6eaf056d224753d59e74182bc4dfe3f7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/cc/dc/17/84258b27a04cd38ac91998abe148203720ca696186635db694\n",
            "Successfully built ktrain keras-bert langdetect networkx seqeval keras-transformer keras-pos-embd keras-multi-head keras-layer-normalization keras-position-wise-feed-forward keras-embed-sim keras-self-attention\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: scikit-learn, keras-pos-embd, keras-self-attention, keras-multi-head, keras-layer-normalization, keras-position-wise-feed-forward, keras-embed-sim, keras-transformer, keras-bert, langdetect, cchardet, networkx, seqeval, ktrain\n",
            "  Found existing installation: scikit-learn 0.22.1\n",
            "    Uninstalling scikit-learn-0.22.1:\n",
            "      Successfully uninstalled scikit-learn-0.22.1\n",
            "  Found existing installation: networkx 2.4\n",
            "    Uninstalling networkx-2.4:\n",
            "      Successfully uninstalled networkx-2.4\n",
            "Successfully installed cchardet-2.1.5 keras-bert-0.80.0 keras-embed-sim-0.7.0 keras-layer-normalization-0.14.0 keras-multi-head-0.22.0 keras-pos-embd-0.11.0 keras-position-wise-feed-forward-0.6.0 keras-self-attention-0.41.0 keras-transformer-0.31.0 ktrain-0.7.3 langdetect-1.0.7 networkx-2.3 scikit-learn-0.21.3 seqeval-0.0.12\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask) (7.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask) (2.10.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask) (1.1.1)\n",
            "Collecting flask-ngrok\n",
            "  Downloading https://files.pythonhosted.org/packages/af/6c/f54cb686ad1129e27d125d182f90f52b32f284e6c8df58c1bae54fa1adbc/flask_ngrok-0.0.25-py3-none-any.whl\n",
            "Requirement already satisfied: Flask>=0.8 in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (1.1.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from flask-ngrok) (2.21.0)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (7.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (2.10.3)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (0.16.0)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from Flask>=0.8->flask-ngrok) (1.1.0)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2.8)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->flask-ngrok) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->Flask>=0.8->flask-ngrok) (1.1.1)\n",
            "Installing collected packages: flask-ngrok\n",
            "Successfully installed flask-ngrok-0.0.25\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AaSIV7v7wBpm",
        "colab_type": "text"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T10:29:49.536507Z",
          "start_time": "2020-01-11T10:29:49.478260Z"
        },
        "id": "CEyVW0YxwBpr",
        "colab_type": "code",
        "outputId": "5ce3cd38-08a0-4373-b61f-c6bada4a8ac1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        }
      },
      "source": [
        "import praw\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string\n",
        "from tqdm import tqdm\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "import ktrain\n",
        "from ktrain import text\n",
        "\n",
        "from flask import Flask\n",
        "from flask import request\n",
        "import re"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "using Keras version: 2.2.4-tf\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsRYhJMGwBqa",
        "colab_type": "text"
      },
      "source": [
        "### Constants"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T21:17:04.111866Z",
          "start_time": "2020-01-10T21:17:04.094964Z"
        },
        "id": "BJ8WoeQKwBqg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "MY_CLIENT_ID = \"Z_U0Fc057pEKJA\"\n",
        "MY_CLIENT_SECRET = \"gDcrvRJaOI8mG97lMLkBehQT_qw\"\n",
        "MY_USER_AGENT = \"local_python:com.task.mysubredditanalysis (by u/reddit_bert_user)\"\n",
        "ALL_PUNCT = string.punctuation\n",
        "N_POSTS=100\n",
        "MAX_BERT_LENGTH=512\n",
        "VAL_PERC=0.25\n",
        "SUBREDDIT_LIST = ['datascience', 'askpsychology', 'startups', 'FanTheories', 'hockey', 'dogs', \n",
        "                  'SecurityAnalysis', 'askscience', 'AskHistorians', 'askmath']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PeEXRd7rwBrW",
        "colab_type": "text"
      },
      "source": [
        "### Task 1 - Data Collection\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "damVGTGfwBre",
        "colab_type": "text"
      },
      "source": [
        "#### Define Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T20:23:59.664448Z",
          "start_time": "2020-01-10T20:23:59.615006Z"
        },
        "id": "k3lpvHJWwBrm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def validate_post(my_post_title, my_post_text, my_post_list):\n",
        "    \"\"\"This function checks if the new post has text and then appends to the list of posts.\"\"\"\n",
        "    if my_post_text != '':\n",
        "            if my_post_title[-1] in ALL_PUNCT:\n",
        "                my_post_list.append(my_post_title + ' ' + my_post_text)\n",
        "            else:\n",
        "                my_post_list.append(my_post_title + '. ' + my_post_text)           \n",
        "    return my_post_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T20:24:00.555198Z",
          "start_time": "2020-01-10T20:24:00.487551Z"
        },
        "id": "2gWGJa_OwBr-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_n_subreddit_top_posts(subreddit_name, n_posts, reddit_client_id = MY_CLIENT_ID, \n",
        "                              reddit_client_secret = MY_CLIENT_SECRET, \n",
        "                              reddit_user_agent=MY_USER_AGENT):\n",
        "    \"\"\"This function returns the top number of posts from a given subreddit from Reddit.\n",
        "    subreddit_name: The name of the subreddit we want posts from\n",
        "    n_posts: The number of top posts from the subreddit\n",
        "    reddit_client_id: The client id of the Reddit developer\n",
        "    reddit_user_agent: The user agent of the Reddit developer\"\"\"\n",
        "    \n",
        "    reddit_instance = praw.Reddit(client_id=MY_CLIENT_ID,\n",
        "                     client_secret=MY_CLIENT_SECRET,\n",
        "                     user_agent=MY_USER_AGENT )\n",
        "    my_subreddit = reddit_instance.subreddit(subreddit_name)\n",
        "    post_list=[]\n",
        "    top_post_generator = my_subreddit.top(limit=None)\n",
        "    for my_post in top_post_generator:\n",
        "        post_title = my_post.title\n",
        "        post_text = my_post.selftext\n",
        "        post_list = validate_post(post_title, post_text, post_list)\n",
        "        \n",
        "        if len(post_list) >= n_posts:\n",
        "            break\n",
        "    return post_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T20:25:15.298662Z",
          "start_time": "2020-01-10T20:25:15.190139Z"
        },
        "id": "BjJBcbhrwBsP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_df_of_posts(my_subreddit_list, n_posts):\n",
        "  \"\"\"This function creates a dataframe with all the posts from the various subreddits.\"\"\"\n",
        "  df_dict = {'subreddit_category': [], 'reddit_post': []}\n",
        "  my_df = pd.DataFrame(data=df_dict)\n",
        "  \n",
        "  for my_subreddit_name in tqdm(my_subreddit_list):\n",
        "      post_list = get_n_subreddit_top_posts(my_subreddit_name, n_posts)\n",
        "      df_b_dict = {'subreddit_category': [my_subreddit_name]*len(post_list), 'reddit_post': post_list}\n",
        "      df_b=pd.DataFrame(data=df_b_dict)\n",
        "      my_df=pd.concat([my_df, df_b], axis=0).reset_index().drop(['index'], axis=1)\n",
        "\n",
        "  return my_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HzlQgSQAwBsp",
        "colab_type": "text"
      },
      "source": [
        "#### Download All the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T21:19:26.787538Z",
          "start_time": "2020-01-10T21:17:51.734562Z"
        },
        "id": "X40_0dK9wBsv",
        "colab_type": "code",
        "outputId": "100dc86e-8bea-4dce-87ea-a69feb4f4429",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "new_df = get_df_of_posts(SUBREDDIT_LIST, N_POSTS)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:57<00:00,  5.54s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-10T21:19:35.826845Z",
          "start_time": "2020-01-10T21:19:35.776476Z"
        },
        "id": "gCUK8iRlwBtV",
        "colab_type": "code",
        "outputId": "492e3004-9551-4520-e8c2-fd2fdd957750",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "new_df.subreddit_category.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "askmath             100\n",
              "dogs                100\n",
              "hockey              100\n",
              "FanTheories         100\n",
              "AskHistorians       100\n",
              "askscience          100\n",
              "datascience         100\n",
              "askpsychology       100\n",
              "SecurityAnalysis    100\n",
              "startups            100\n",
              "Name: subreddit_category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jc4ey6Vxet33",
        "colab_type": "text"
      },
      "source": [
        "#### Encode the text classes into numbers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T11:12:04.356966Z",
          "start_time": "2020-01-11T11:12:04.151793Z"
        },
        "id": "zfde5jjlwBuN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_encoder = LabelEncoder()\n",
        "my_encoder.fit(SUBREDDIT_LIST)\n",
        "new_df.subreddit_category = my_encoder.transform(new_df.subreddit_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T11:12:22.281011Z",
          "start_time": "2020-01-11T11:12:22.202057Z"
        },
        "id": "8UnGdLD4wBub",
        "colab_type": "code",
        "outputId": "f5b07fe5-e915-494b-cc85-99b92f04a1e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "new_df.subreddit_category.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9    100\n",
              "8    100\n",
              "7    100\n",
              "6    100\n",
              "5    100\n",
              "4    100\n",
              "3    100\n",
              "2    100\n",
              "1    100\n",
              "0    100\n",
              "Name: subreddit_category, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_O4wxbNwBut",
        "colab_type": "text"
      },
      "source": [
        "#### Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T11:36:47.957830Z",
          "start_time": "2020-01-11T11:36:47.853089Z"
        },
        "id": "e_TcP8JlwBuy",
        "colab_type": "code",
        "outputId": "e3da9444-de27-4854-e720-a57eab0595d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "X = new_df.drop(['subreddit_category'], axis=1)\n",
        "y = new_df.subreddit_category\n",
        "    \n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "original_size = new_df.shape[0]\n",
        "train_size = X_train.shape[0]\n",
        "test_size = X_test.shape[0]\n",
        "\n",
        "print(\"Size of Train set: {}%\".format(train_size/original_size*100))\n",
        "print(\"Size of Test set: {}%\".format(test_size/original_size*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Size of Train set: 80.0%\n",
            "Size of Test set: 20.0%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8OqymprwBvU",
        "colab_type": "text"
      },
      "source": [
        "### Task 2 - Classification Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzZN08U-wBvZ",
        "colab_type": "text"
      },
      "source": [
        "#### Preprocess the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T11:39:04.174733Z",
          "start_time": "2020-01-11T11:38:27.237972Z"
        },
        "scrolled": true,
        "id": "-HY9awdzwBv4",
        "colab_type": "code",
        "outputId": "196ca740-1372-467e-dfcd-90a205439a64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        }
      },
      "source": [
        "(X_train,  y_train), (X_test, y_test), preproc = text.texts_from_array(x_train=X_train.reddit_post.to_list(), y_train=y_train.to_list(), \n",
        "                                                                       x_test=X_test.reddit_post.to_list(), y_test=y_test.to_list(), \n",
        "                                                                       val_pct=VAL_PERC,\n",
        "                                                                       class_names=list(my_encoder.classes_),\n",
        "                                                                       preprocess_mode='bert',\n",
        "                                                                       maxlen=MAX_BERT_LENGTH) #The max length bert supports is 512 characters."
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading pretrained BERT model (uncased_L-12_H-768_A-12.zip)...\n",
            "[██████████████████████████████████████████████████]\n",
            "extracting pretrained BERT model...\n",
            "done.\n",
            "\n",
            "cleanup downloaded zip...\n",
            "done.\n",
            "\n",
            "preprocessing train...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "preprocessing test...\n",
            "language: en\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "done."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O6t0AsgyL5El",
        "colab_type": "text"
      },
      "source": [
        "##### Create and fit the BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-01-11T11:53:24.554303Z",
          "start_time": "2020-01-11T11:49:49.965462Z"
        },
        "id": "JX1vJ9lfwBwm",
        "colab_type": "code",
        "outputId": "d3bd9c5b-09ed-4fd4-a479-937b9b65aaa5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "model = text.text_classifier('bert', train_data=(X_train, y_train), preproc=preproc)\n",
        "learner = ktrain.get_learner(model, train_data=(X_train, y_train), batch_size=6, val_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Is Multi-Label? False\n",
            "maxlen is 512\n",
            "done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "ExecuteTime": {
          "start_time": "2020-01-11T11:54:43.577Z"
        },
        "id": "HIYokJGkwBw6",
        "colab_type": "code",
        "outputId": "df5562f0-0680-460c-bdca-cc44d5600ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "learner.fit_onecycle(2e-5, 4)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "begin training using onecycle policy with max lr of 2e-05...\n",
            "Train on 800 samples, validate on 200 samples\n",
            "Epoch 1/4\n",
            "800/800 [==============================] - 77s 97ms/sample - loss: 2.0808 - acc: 0.2925 - val_loss: 1.4011 - val_acc: 0.6450\n",
            "Epoch 2/4\n",
            "800/800 [==============================] - 68s 85ms/sample - loss: 0.8393 - acc: 0.8062 - val_loss: 0.4423 - val_acc: 0.8750\n",
            "Epoch 3/4\n",
            "800/800 [==============================] - 68s 85ms/sample - loss: 0.2203 - acc: 0.9550 - val_loss: 0.3002 - val_acc: 0.8900\n",
            "Epoch 4/4\n",
            "800/800 [==============================] - 68s 85ms/sample - loss: 0.0800 - acc: 0.9925 - val_loss: 0.2840 - val_acc: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f1b7a362b70>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qH0XArYTx8ev",
        "colab_type": "text"
      },
      "source": [
        "We can see from the above epochs that the validation accuracy stops increasing after the 4th epoch and the training accuracy reaches 0.99 which is close to the highest possible. So further increasing epochs would results in overfitting. Therefore we stop at 4 epochs."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0g60csZZMGcV",
        "colab_type": "text"
      },
      "source": [
        "#### Evaluate BERT Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R2eMLiqw2y5R",
        "colab_type": "code",
        "outputId": "d93ce050-e373-4131-8885-483623a022c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 476
        }
      },
      "source": [
        "learner.validate(val_data=(X_test, y_test), class_names=list(my_encoder.classes_))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "                  precision    recall  f1-score   support\n",
            "\n",
            "   AskHistorians       0.89      1.00      0.94        17\n",
            "     FanTheories       1.00      1.00      1.00        24\n",
            "SecurityAnalysis       0.82      0.82      0.82        17\n",
            "         askmath       0.82      0.90      0.86        20\n",
            "   askpsychology       0.89      0.81      0.85        21\n",
            "      askscience       0.85      0.77      0.81        22\n",
            "     datascience       0.95      0.91      0.93        22\n",
            "            dogs       1.00      1.00      1.00        21\n",
            "          hockey       0.94      1.00      0.97        17\n",
            "        startups       0.79      0.79      0.79        19\n",
            "\n",
            "        accuracy                           0.90       200\n",
            "       macro avg       0.90      0.90      0.90       200\n",
            "    weighted avg       0.90      0.90      0.90       200\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[17,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0, 24,  0,  0,  0,  0,  0,  0,  0,  0],\n",
              "       [ 0,  0, 14,  1,  0,  0,  0,  0,  0,  2],\n",
              "       [ 0,  0,  0, 18,  0,  1,  0,  0,  1,  0],\n",
              "       [ 0,  0,  0,  1, 17,  2,  0,  0,  0,  1],\n",
              "       [ 2,  0,  0,  1,  2, 17,  0,  0,  0,  0],\n",
              "       [ 0,  0,  0,  1,  0,  0, 20,  0,  0,  1],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0, 21,  0,  0],\n",
              "       [ 0,  0,  0,  0,  0,  0,  0,  0, 17,  0],\n",
              "       [ 0,  0,  3,  0,  0,  0,  1,  0,  0, 15]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MrhsPlbvMaLk",
        "colab_type": "text"
      },
      "source": [
        "### Create and save predictor to be Served"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yVH56Gsp37wc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor = ktrain.get_predictor(learner.model, preproc)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2-ANVU55J6J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "predictor.save('my_predictor')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PuIB8sacWe89",
        "colab_type": "text"
      },
      "source": [
        "### Task 3 - Model Serving (Bonus Task)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDxfWALkPRYk",
        "colab_type": "code",
        "outputId": "6d0a3ff2-31a7-470f-810a-a140505169f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "from flask_ngrok import run_with_ngrok\n",
        "from flask import Flask\n",
        "\n",
        "app = Flask(__name__)\n",
        "run_with_ngrok(app)   #starts ngrok when the app is run@app.route(\"/\")\n",
        "\n",
        "@app.route('/classify_text/<name>')\n",
        "def model_to_serve(name):\n",
        "  \"\"\"This function takes a string as input and predicts the subreddit it came from\"\"\"\n",
        "  predictor = ktrain.load_predictor('my_predictor')\n",
        "  my_prediction = predictor.predict(name)\n",
        "  return f\"I consider it being {my_prediction}\"\n",
        "\n",
        "@app.route(\"/\")\n",
        "def home():\n",
        "    return \"<h1>Go to /classify_text/ to make it run</h1>\"\n",
        "  \n",
        "app.run()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " * Serving Flask app \"__main__\" (lazy loading)\n",
            " * Environment: production\n",
            "   WARNING: This is a development server. Do not use it in a production deployment.\n",
            "   Use a production WSGI server instead.\n",
            " * Debug mode: off\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://127.0.0.1:5000/ (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            " * Running on http://0f69ebe0.ngrok.io\n",
            " * Traffic stats available on http://127.0.0.1:4040\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/Jan/2020 23:36:46] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/Jan/2020 23:36:46] \"\u001b[33mGET /favicon.ico HTTP/1.1\u001b[0m\" 404 -\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              ""
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "127.0.0.1 - - [12/Jan/2020 23:37:23] \"\u001b[37mGET /classify_text/owls HTTP/1.1\u001b[0m\" 200 -\n",
            "127.0.0.1 - - [12/Jan/2020 23:37:43] \"\u001b[33mGET /classify_text/ HTTP/1.1\u001b[0m\" 404 -\n",
            "127.0.0.1 - - [12/Jan/2020 23:41:00] \"\u001b[37mGET / HTTP/1.1\u001b[0m\" 200 -\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLpAMjLfzZpB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}